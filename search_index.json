[["index.html", "Portfolio Naresh Home Page Introduction", " Portfolio Naresh Naresh de Bruijn 2024-05-16 Home Page Introduction Welcome to my portfolio book! Within these pages, you’ll find my assignments exploring the intricate interplay between data science and the life sciences. Table of Contents Home Page Resumé Free schedule plan Guerrilla Analytics C.elegans plate experiment Reproducible Research Projecticum: Pediatric Cancer References Packages dependency dslabs fs ggplot2 here RColorBrewer reactable readxl tibble tidyverse "],["id_01.html", "Resumé Personal details Allow me to introduce myself briefly Education Internship / (major) project Relevant Coursework Experiences", " Resumé Personal details Name: Naresh Surname: de Bruijn Gender: Male Date of birth: 02-06-1999 Position: Medical Data Science Student Contact HU mail Linked-In Github Portfolio page Allow me to introduce myself briefly I am deeply passionate about learning about cancer and continually seek opportunities to expand my knowledge in this area. I possess strong analytical skills and expertise in R and Bash coding. I have actively engaged in various initiatives related to cancer research. This includes my internship at Hartwig Medical Foundation, as well as projects at Hubrecht Institute, and Princess Maxima Centre. Through these experiences, I have developed a deep understanding of the complexities of cancer and the importance of data‐driven approaches in advancing our understanding and treatment options. Education Internship / (major) project Relevant Coursework Experiences "],["id_02.html", "Free schedule plan Introduction", " Free schedule plan Introduction In about two years, I picture myself completing my master’s program in Medical Science. My goal is to actively participate in an oncology research group, utilizing bioinformatics tools to propel advancements in the field of oncology. Currently, I am making steady progress toward this objective. Drawing on my background in Life Sciences and a growing interest in data science applied to biology, I am well-equipped to make meaningful contributions to oncology research through bioinformatics. To continue on this path, I recognize the importance of seizing educational opportunities, gaining hands-on research experience, and expanding my professional network within the relevant fields. In terms of skill development, I have chosen to explore the Bisulfite sequencing technique (BS-seq) during the “Data Science for Biology 2” course. This decision reflects the pivotal role of BS-seq in epigenomics research, where it is instrumental in uncovering DNA methylation patterns and their implications for gene regulation and disease. By mastering BS-seq techniques, I aim to contribute significantly to the advancement of understanding epigenetic mechanisms and their diverse biological implications. Goals Understanding the Bisulfite conversion technique Exploring new bioinformatic packages and tools Understanding the workflow of the BS-seq Plan I’ll spend 32 hours on my free schedule to begin learning my new skill in BS-seq in across 4 days in two weeks. in the table 1 you will find my planning for the 4 days. Table 1: BS-seq analysis plan Day Activity Note 1 4hrs paper search 4hrs reading papers get to know the technique of bisulfite conversion and the sequencing technique 2 2hrs draw pipeline 2hrs 4hrs finding data and code try to understand the analysis collect datasets from e.g,. GEO 3 2 hrs collecting code and packages 2hrs commenting codes 4 hrs creating a pipeline find out if there are packages available comment codes for better understanding of the code craete a pipeline in a form of Rmarkdown or Rpackage 4 8 hrs wrangling code for analysis Try to mimic the analysis of the author "],["id_03.html", "Guerrilla Analytics", " Guerrilla Analytics Example Guerrilla Analytics DAUR-II knitr::include_graphics(here::here( &quot;documents&quot;, &quot;images&quot;, &quot;Guerrilla_Analytics_metagenomics.png&quot; )) Figure 1: Guerrilla Analytics tree structure of DAUR-II. Root of Workflow portfolio project # print root of workflows portfolio project fs::dir_tree(&quot;.&quot;, recurse = T, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = T) ## . ## ├── .gitignore ## ├── 01_CV.Rmd ## ├── 02_Free_schedule_plan.Rmd ## ├── 03_Guerrilla_Analytics.Rmd ## ├── 04_Celegans_plate_experiment.Rmd ## ├── 05_Reproducible_Research.Rmd ## ├── 07_Projecticum_introduction.Rmd ## ├── 20_ref.Rmd ## ├── DSFB2_portfolio.Rproj ## ├── LICENSE ## ├── README.md ## ├── code ## │ ├── README.txt ## │ ├── SNPextract.sh ## │ └── fastqc.sh ## ├── data ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── METADATA.txt ## │ ├── README.txt ## │ └── fastqc_output ## │ ├── SRR9834204_fastqc.html ## │ └── SRR9834204_fastqc.zip ## ├── documents ## │ ├── .DS_Store ## │ ├── README.txt ## │ ├── bibliography.bib ## │ └── images ## │ ├── .DS_Store ## │ ├── DNA_helix_blue.png ## │ ├── DNA_helix_image.png ## │ ├── DNA_helix_logo.png ## │ ├── Guerrilla_Analytics_metagenomics.png ## │ └── resume_pic.png ## ├── hero-image.html ## ├── layout.css ## └── raw_data ## ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## ├── README.txt ## └── SRR9834204.fastq.gz Root of Projecticum: Peadiatric Cancer project # print root of Projecticum project repo fs::dir_tree(here::here(&quot;../DSFB2_boys/&quot;), recurse = T, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all =F) ## /Users/naresh/Documents/08_DSFB/DSFB2_portfolio/../DSFB2_boys/ ## ├── Conflict4_2 ## ├── DSFB2_Project.Rproj ## ├── DSFB2_boys.Rproj ## ├── LICENSE ## ├── README.html ## ├── README.md ## ├── code ## │ ├── Global.R ## │ ├── server.R ## │ └── ui.R ## ├── documents ## │ ├── Project_description.txt ## │ ├── bibliography.bib ## │ ├── images ## │ │ ├── logo.png ## │ │ ├── maxima_logo.png ## │ │ └── screenshots ## │ │ └── dir_tree_project.png ## │ ├── logo.tex ## │ ├── project_introduction.Rmd ## │ └── project_introduction.pdf ## ├── raw_data ## │ ├── Project_description.txt ## │ ├── README.md ## │ ├── code ## │ │ ├── Global.R ## │ │ ├── server.R ## │ │ └── ui.R ## │ ├── liquid_biopsies_data ## │ │ ├── HHUBM000AAA_HHUBM000CAA_HHUCZ000AAA_WXS.vcf ## │ │ ├── HHUBM000AAA_HHUCZ000AAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000ACA_HHUBM000ABA_HHUCZ000ACA_WXS.vcf ## │ │ ├── HHUBM000ACA_HHUCZ000ACA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000ADA_HHUBM000CAA_HHUCZ000ADA_WXS.vcf ## │ │ ├── HHUBM000ADA_HHUCZ000ADA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000BAA_HHUBM000CAA_HHUCZ000BAA_WXS.vcf ## │ │ ├── HHUBM000BAA_HHUCZ000BAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000EAA_HHUBM000FAA_HHUCZ000EAA_WXS.vcf ## │ │ ├── HHUBM000EAA_HHUCZ000EAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000GAA_HHUBM000IAA_HHUCZ000GAA_WXS.vcf ## │ │ ├── HHUBM000GAA_HHUCZ000GAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000HAA_HHUBM000IAA_HHUCZ000HAA_WXS.vcf ## │ │ ├── HHUBM000HAA_HHUCZ000HAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000KAA_HHUBM000LAA_HHUCZ000KAA_WXS.vcf ## │ │ ├── HHUBM000KAA_HHUCZ000KAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000YAA_HHUBM000ABA_HHUCZ000YAA_WXS.vcf ## │ │ ├── HHUBM000YAA_HHUCZ000YAA_WXS.tumor.cnv.vcf.gz ## │ │ ├── HHUBM000ZAA_HHUBM000ABA_HHUCZ000ZAA_WXS.vcf ## │ │ └── HHUBM000ZAA_HHUCZ000ZAA_WXS.tumor.cnv.vcf.gz ## │ ├── readme.txt ## │ ├── resources ## │ │ ├── 20220628_gene_list.txt ## │ │ ├── Genetic_aberrations_Wilms_tumor_20230328.csv ## │ │ ├── RMS_gene_list.tsv ## │ │ ├── fake_ascat_results.tsv ## │ │ ├── maf_cfDNA_list.rds ## │ │ ├── maf_tumor_list.rds ## │ │ └── trecode_info_byPMCID_inSampleInfo.tsv ## │ └── scripts ## │ └── functions.R ## ├── resources ## │ ├── 20220628_gene_list.txt ## │ ├── Genetic_aberrations_Wilms_tumor_20230328.csv ## │ ├── RMS_gene_list.tsv ## │ ├── fake_ascat_results.tsv ## │ ├── maf ## │ │ ├── maf_cfDNA_list.rds ## │ │ └── maf_tumor_list.rds ## │ └── trecode_info_byPMCID_inSampleInfo.tsv ## └── scripts ## └── functions.R "],["id_04.html", "C.elegans plate experiment Scatterplot", " C.elegans plate experiment Import and inspect dataset # import C.elegans data file CE_LIQ_FLOW_062_Tidydata &lt;- read_excel( here::here( &quot;raw_data&quot;, &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) # inspecting data in table format with the reactable reactable(data = CE_LIQ_FLOW_062_Tidydata, defaultPageSize = 5, compact = T, highlight = T) Scatterplot Pseudocode Decide which columns will be included in the scatterplot Checking and if needed change the datatypes of columns Plot the data in a scatterplot using ggplot Normalise y-axis counts Set the x-axis to a log10 scale Use Jitter to avoid overlaps of points # inspect the type of data of column RawData, compName, and compConcentration CE_LIQ_FLOW_062_Tidydata %&gt;% select( RawData, compName, compConcentration, expType) %&gt;% map(class) ## $RawData ## [1] &quot;numeric&quot; ## ## $compName ## [1] &quot;character&quot; ## ## $compConcentration ## [1] &quot;character&quot; ## ## $expType ## [1] &quot;character&quot; # Change &quot;compConcentration&quot; to numeric CE_LIQ_FLOW_062_Tidydata$compConcentration &lt;- parse_number(CE_LIQ_FLOW_062_Tidydata$compConcentration) # change &quot;compName&quot; into a factor CE_LIQ_FLOW_062_Tidydata$compName &lt;- factor(CE_LIQ_FLOW_062_Tidydata$compName, levels = unique(CE_LIQ_FLOW_062_Tidydata$compName)) # change &quot;expType&quot; into a factor CE_LIQ_FLOW_062_Tidydata$expType &lt;- factor(CE_LIQ_FLOW_062_Tidydata$expType, levels = unique(CE_LIQ_FLOW_062_Tidydata$expType)) # Checking the new dataypes and factor levels CE_LIQ_FLOW_062_Tidydata %&gt;% select( RawData, compName, compConcentration, expType) %&gt;% map(class) ## $RawData ## [1] &quot;numeric&quot; ## ## $compName ## [1] &quot;factor&quot; ## ## $compConcentration ## [1] &quot;numeric&quot; ## ## $expType ## [1] &quot;factor&quot; # check levels of &quot;compName&quot; and &quot;expType&quot; CE_LIQ_FLOW_062_Tidydata %&gt;% select( compName, expType) %&gt;% map(levels) ## $compName ## [1] &quot;2,6-diisopropylnaphthalene&quot; &quot;decane&quot; ## [3] &quot;naphthalene&quot; &quot;Ethanol&quot; ## [5] &quot;S-medium&quot; ## ## $expType ## [1] &quot;experiment&quot; &quot;controlPositive&quot; &quot;controlNegative&quot; &quot;controlVehicleA&quot; Scatterplot CE_LIQ_FLOW_062_Tidydata # plot dataset in a scatterplot CE_LIQ_FLOW_062_Tidydata %&gt;% ggplot(aes(x = log10(compConcentration + .001), y = RawData))+ geom_point(aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.1), # jitter to avoid overlap na.rm = T)+ labs( title = &quot;Scatterplot CE_LIQ_FLOW_062_Tidydata&quot;, x = &quot;log10(compConcentration) in nM&quot;, y = &quot;RawData in counts&quot;)+ theme_bw(base_size = 12)+ scale_color_brewer(palette = &quot;Dark2&quot;) # colour-blind friendly colours Figure 2: In this experiment, we have two control groups: the positive control for this experiments is controlPositive (in triangles). The negative control for this experiment is controlNegative (in squares). Analysis plan Pseudocode Analysing Compound Concentrations: Load the data (import dataset). Filter the data to separate concentrations by compound (filter concentrations). Create a bar graph representing the concentrations for each compound, with standard deviation bars as error bars (plotting). Conduct a Shapiro-Wilk test separately for each group to assess normality (normality check). Perform an analysis of variance (ANOVA) test to determine if there are significant differences between the compound groups (ANOVA test). If the ANOVA test yields significance, proceed with post hoc tests to compare specific groups (post hoc test). Draw conclusions based on the results obtained (conclusion). Comparing LC50 Curves: Load the LC50 data (import dataset). Filter the data to obtain LC50 concentrations for each compound (filter concentration). Plot the LC50 curves for each compound (plotting). Calculate the 95% confidence intervals for each LC50 curve (95% confidence interval). Assess the overlap between the 95% confidence intervals of different compounds (check overlap). Draw conclusions based on the comparison of LC50 curves and the overlap of confidence intervals (conclusion). Normalise CE_LIQ_FLOW_062_Tidydata # filter on &quot;controlNegative&quot; data CE_LIQ_FLOW_062_contNeg &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% filter(expType == &quot;controlNegative&quot;) # calculate mean of &quot;RawData&quot; of the filtered dataset contNeg_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg$RawData) contNeg_mean # check contNeg_mean ## [1] 85.9 # create a normalised column with the normalised values. CE_LIQ_FLOW_062_Tidydata_norm &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% mutate(Normalised = RawData/mean(contNeg_mean)) # mean RawData mean(CE_LIQ_FLOW_062_Tidydata_norm$RawData, na.rm=T) ## [1] 68.10423 # check the normalisation values are a fraction of 1 CE_LIQ_FLOW_062_contNeg_norm &lt;- CE_LIQ_FLOW_062_Tidydata_norm %&gt;% filter(expType == &quot;controlNegative&quot;) # check if mean value of controlNegative is equal to &quot;1&quot; contNegNorm_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg_norm$Normalised) contNegNorm_mean ## [1] 1 # check if the normalised values are a fraction of &quot;1&quot;. CE_LIQ_FLOW_062_Tidydata_norm %&gt;% select(compName, expType, compConcentration, RawData, Normalised) %&gt;% reactable( defaultPageSize = 5, compact = T, highlight = T, columns = list( Normalised = colDef(minWidth = 200) )) Scatterplot normalised CE_LIQ_FLOW_062_Tidydata CE_LIQ_FLOW_062_Tidydata_norm %&gt;% ggplot(aes(x = log10(compConcentration + .001), y = Normalised))+ geom_point(aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.1), # jitter to avoid overlap na.rm = T)+ labs( title = &quot;Scatterplot normalised CE_LIQ_FLOW_062_Tidydata&quot;, y = &quot;Normalised RawData counts&quot;, x = &quot;log10(compConcentration) in nM&quot;)+ theme_bw(base_size = 12)+ scale_color_brewer(palette = &quot;Dark2&quot;) # colour-blind friendly colours Figure 3: In this experiment, we have two control groups: the positive control, labeled as “controlPositive”, and the negative control, labeled as “controlNegative”. The data is normalised with the negativeControl mean. "],["id_05.html", "Open Peer Review", " Open Peer Review Introduction The COVID-19 pandemic has underscored the critical role of open science and reproducible research. Open science facilitates the free exchange of findings, while reproducible research ensures that publications are both accessible and verifiable, thereby enhancing transparency and trust in scientific conclusions. Together, these practices drive scientific progress forward [1]. The transparency criteria of Reproducibility and reporting practices in COVID-19 preprint manuscripts was used to examine the reproducibility of an published article. I’ll peer review Applying Cell Painting in Non-Tumorigenic Breast Cells to Understand Impacts of Common Chemical Exposures from bioRxiv using the transparency criteria shown in table 2. Article overview Researchers used Cell Painting to study the effects of various chemicals on mammary cells to understand their potential links to breast cancer. They examined 16 chemicals and 21 small molecules, analysing over 3000 cellular features. By comparing their findings with human exposure data, they identified similarities between certain chemicals and known pathways. Notably, a pesticide metabolite showed similarities to a Wnt pathway activator, suggesting a potential mechanism for breast cancer risk. This study highlights Cell Painting’s utility in understanding the effects of common environmental toxins on breast cancer[2]. Peer review Table 2. The Ripeta team analyzed eight reproducibility criteria [1]. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. yes Data Location Where the article’s data can be accessed, either raw or processed. yes; all in paper or supplementary fils Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. no Author Review The professionalism of the contact information that the author has provided in the manuscript. Tier 3 Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. no Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. no Conclusion This article meets most criteria at a sufficient level. However, it falls short in a couple of key areas. Firstly, it does not specify the study locations in the methods section. Secondly, there is no code available for the analysis, hindering the replicability of the results. These shortcomings could impact the article’s reception in open peer review processes, highlighting areas for improvement in transparency and reproducibility. Furthermore, there was no ethics statement mentioned. This may not be relevant in this study, but it should be included in the ethical statement section. With these shortcomes, the article does not pass the open peer review. Reference "],["id_07.html", "Projecticum: Pediatric Cancer Introduction", " Projecticum: Pediatric Cancer Introduction the past decade, we have witnessed a change from traditional invasive techniques for diagnosing and monitoring cancer to more innovative and non-invasive methods, such as liquid biopsies. This new approach has revolutionized the field of clinical oncology, offering significant benefits such as simplified tumor sampling and personalized therapeutic strategies [3]. Liquid biopsies involve the isolation and analysis of tumor-derived samples, such as circulating tumor cells (CTC) and circulating tumor DNA (ctDNA), from the bodily fluids of cancer patients. Once these biomarkers are extracted, they provide invaluable information about tumor characteristics such as progression, staging, heterogeneity, gene mutations, and clonal evolution. Thus, liquid biopsies have improved our understanding of tumors, leading to better detection, ongoing monitoring, and personalized treatment [3]. cfDNA Cell-free DNA (cfDNA) is a term used to describe small fragments of DNA that circulate freely in bodily fluids such as blood, urine, and cerebrospinal fluid. Unlike DNA contained within intact cells, cfDNA is released into the bloodstream through various physiological processes, including apoptosis, necrosis, and active secretion [4]. The origins of cfDNA are diverse and can include contributions from normal cells, as well as cells undergoing pathological processes such as cancer. In the context of liquid biopsy, which involves the analysis of biomarkers in bodily fluids for diagnostic or monitoring purposes, cfDNA holds particular significance [3, 5]. In cancer patients, cfDNA can contain genetic mutations or other alterations that are characteristic of the tumor. These small fragments of DNA are called ctDNA. By isolating and analyzing cfDNA from a blood sample, clinicians can gain valuable insights into the genetic profile of a patient’s tumor, potentially facilitating early detection, monitoring of disease progression, assessment of treatment response, and detection of resistance mutations [5]. R shiny app The R shiny app: A place where you can “make an interactive web application that executes R code on the backend”. The functionality and user friendly interactivity of the R shiny is needed for our project for 2 main reasons. When looking at data, you need to be able to visualize it. Then when this data is visualized, you are still able to give user inputs that can adjust and modify the visualization and results that you are looking for [6]. When working with the ct- and cf-DNA, we want to be able to really visualize and capture what this data means for us. But in the mean time we also would like to give inputs for our charts and graphs, that makes it so that we can really pinpoint on the things we are sought after. By having a web application that can run R calculations on the background, we can create an user friendly environment, that can be used by more than just data scientist. Reference "],["id_20.html", "Reference", " Reference "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
